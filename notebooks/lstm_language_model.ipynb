{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgZQvsyfCwHC"
   },
   "source": [
    "## Implementación de un modelo de lenguaje a nivel de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SamdZayOCwHD",
    "outputId": "243a6454-4765-486a-f7fe-8733afcb3c2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "# en este ejemplo, se usa éste texto...\n",
    "path = '../data/nietzsche.txt'\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMEFVX96CwHJ"
   },
   "source": [
    "Definimos las secuencias, vocabulario y demás cosas para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bt_Eva4JCwHK",
    "outputId": "78bdaea0-d339-42b0-f52d-5572d89d8db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200278\n",
      "Unique characters: 57\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDkQNsKTCwHO"
   },
   "source": [
    "Ahora, considera éste modelo como el baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMjpek8BCwHO",
    "outputId": "616e48b8-5ef7-4503-9385-767eaf7f802a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               95232     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 57)                7353      \n",
      "=================================================================\n",
      "Total params: 102,585\n",
      "Trainable params: 102,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pLSpvy0CwHS"
   },
   "source": [
    "Lo siguiente es entrenar el modelo, dar una secuencia inicial y generar texto a partir de esto. \n",
    "Una modificación para tratar de evitar la repetitividad de las secuencias generadas (sobre todo en las primeras épocas del entrenamiento), es modificar la distribución de probabilidad `softmax` para introducir cierta aleatoriedad en el proceso de muestreo (stochastic sampling). A partir de cierto parámetro (\"temperatura\"), se construye una nueva distribución de probabilidad de las salidas del modelo. El siguiente código implementa ésta idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yu7t9NbBCwHT"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3s-PUo7CwHW"
   },
   "source": [
    "El modelo de lenguaje queda entonces como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8t2ro8rRCwHW"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 60):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.5, 1.0]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x75n2VDiCwHa"
   },
   "source": [
    "### Generando tuits como AMLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNPTLPmXIWoB"
   },
   "source": [
    "- Para los tweets, se realizó un pre-proceso, se pusieron todos los caracteres en minúsculas, se quitaron acentos, se quitaron caracteres duplicados, se cambiaron los Urls por la palabra url al final del tweet, se quitaron posibles emojis o referencias a otros usuarios (@user).\n",
    "\n",
    "- Para el modelo, se probó el baseline con 5 epocas, diferentes optmizadores (RMSprop, SGD, Adam), diferentes tamaños de learning rate (0.001, 0.01, 0.1), diferentes tamaños de minibatch (8-512). Posteriormente se probó con diferente numero de unidades en la capa LSTM (64-256), agregando mas capas LSTM y probando con capa bidireccionales y por ultimo dejando entrenar el modelo hasta obtener una buena generación de texto.\n",
    "\n",
    "- El mejor modelo resultó entrenando por 40 epocas, con optimizador RMSprop, con learning rate 0.01, con tamaño de minibatch de 256, con capa LSTM con 256 unidades y una capa densa con el tamaño de numero de caracteres y función de activación softmax, con un error de entrenamiento de 0.71\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9398,
     "status": "ok",
     "timestamp": 1605113174891,
     "user": {
      "displayName": "victor gomez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7NoO07esRDYdazuGkOEYKuXov9z7pGfDNqJTa3w=s64",
      "userId": "00777037408104386876"
     },
     "user_tz": 360
    },
    "id": "HdoCizPaC3x-",
    "outputId": "e1de7bc9-d715-4ca3-8c8a-2d54cbce068c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_39 (LSTM)               (None, 256)               319488    \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 55)                14135     \n",
      "=================================================================\n",
      "Total params: 333,623\n",
      "Trainable params: 333,623\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# librerias\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "#path\n",
    "path = \"/home/victor/cursos/banxico/nlp_2020/data/\" #path de los archivos\n",
    "\n",
    "#tweets\n",
    "text = open(path+\"amlo_tuits_preprocesados.txt\").read().lower() #carga los tweets procesados\n",
    "\n",
    "#modelo\n",
    "Modelo1 = tf.keras.models.load_model(path+'amlo_tuits.h5', compile=False) #carga el mejor modelo\n",
    "Modelo1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1605113431931,
     "user": {
      "displayName": "victor gomez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7NoO07esRDYdazuGkOEYKuXov9z7pGfDNqJTa3w=s64",
      "userId": "00777037408104386876"
     },
     "user_tz": 360
    },
    "id": "Vk_bGWo4DTx1"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "#Función para generar un tweet de AMLO\n",
    "def generate_AMLO_tweet(model=Modelo1, text=text, temperature=0.3):\n",
    "  maxlen=60\n",
    "  chars = sorted(list(set(text))) #vocabulario o caracteres unicos\n",
    "  char_indices = dict((char, chars.index(char)) for char in chars) #crea un diccionario donde las llaves son los caracteres y los valores son los indices\n",
    "# Select a text seed at random\n",
    "  start_index = random.randint(0, len(text) - maxlen - 1) #selecciona del texto original aleatoriamente una secuencia del mismo tamaño con el que se entrenó\n",
    "  generated_text = text[start_index: start_index + maxlen]\n",
    "  \n",
    "  # We generate 140 characters\n",
    "  i=0\n",
    "  next_char=''\n",
    "  tweet_char=[]\n",
    "  tweet_char.append(generated_text)\n",
    "  while i<140 and next_char != '\\n': #genera 400 caracteres\n",
    "    sampled = np.zeros((1, maxlen, len(chars))) #la semilla la vectoriza\n",
    "    for t, char in enumerate(generated_text):\n",
    "      sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(sampled, verbose=0)[0] #predice con la semilla y obtiene la distribucion de probabilidad\n",
    "    next_index = sample(preds, temperature) #obtiene el siguiente indice del caracter\n",
    "    next_char = chars[next_index] #obtiene el caracter\n",
    "    i+=1\n",
    "    tweet_char.append(next_char)\n",
    "    generated_text += next_char #concatena el caracter\n",
    "    generated_text = generated_text[1:] #da un paso adelante para actualizar la entrada para la proxima prediccion\n",
    "  tweet=\"\".join(ch for ch in tweet_char)  #concatena\n",
    "  print(tweet) #muestra el tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi3ETjRgIwBL"
   },
   "source": [
    "Finalmente se probó con diferentes valores de temperatura (0.2-3), notando que:\n",
    "\n",
    "- Temperatura <0.3 se repetían mucho algunas palabras\n",
    "- 0.3< Temperatura <0.5 se obtenían mejores tweets\n",
    "- 0.5< Temperatura <1 se obtenían algunas palabras raras\n",
    "- 1< Temperatura,  no tenía ningún sentido\n",
    "\n",
    "Algunos ejemplos con temperatura=0.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22393,
     "status": "ok",
     "timestamp": 1605113566533,
     "user": {
      "displayName": "victor gomez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7NoO07esRDYdazuGkOEYKuXov9z7pGfDNqJTa3w=s64",
      "userId": "00777037408104386876"
     },
     "user_tz": 360
    },
    "id": "jBMfbqqNDeMM",
    "outputId": "7c1d3d34-0f01-46e3-a3ce-c29539ae75b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMLO Tweets:\n",
      "\n",
      " tlapacoyan y por la noche en misantla veracruz por eso la gente en el pais no legado se responde a seguir en el pacifica la entrevista que me hizo ciro por los empresarios que impera en chihuahua y l\n",
      "\n",
      "an lazaro cardenas zihuatanejo y vamos de regreso a 20 dias de chiapas en el problema de corupcion para el bienestar de las ciastandos de mexico url\n",
      "\n",
      "\n",
      "es tiera fertil para el genocidio ni para canalas que lo impuesto por la redes social url\n",
      "\n",
      "\n",
      "l a la reforma educativa en toluca se presento a la maestra delfina gomez y trataremos la presidencia en el acto de ser credito presente a la notria de gobierno de la solicitud provotica en el preside\n",
      "\n",
      "ion de registro del partido de calderon url\n",
      " seguiremos cumplia el gobernador de la solidaridad y el presidente de mexico a la gente en el problema de la frontera y aliandre y el problema de la solida\n",
      "\n",
      " mercado a nadie engana y puede resultar aterador repudio la corupcion y el prd y los de la madada del prd url\n",
      "\n",
      "\n",
      "omplejo petroquimico pajaritos impugnaremos emilio lozoya distrucion en la camara de diputada seguirno en chiapas url\n",
      "\n",
      "\n",
      "temente en el senado url\n",
      " tomaremos decisiones con base en puebla y enformar a la gente en el presidente de asambleas de morena asi se aprobaran el aprobo de gasolina en el proyecto de nuestros pueblo\n",
      "\n",
      " a ver hoy a las 9 45 de la noche la segunda parte de la entrevista que me hizo jaaudito de la gente es un ambiente la mafia no se puede esperanza en la campana de la confredada manana es marginarios \n",
      "\n",
      "ico con pueblo pobre van aprobar la nueva ley televisa los pasos de campesinos es un impunidad en el mundo al df para el presidente de mexico a la paz gran para el presidente de mexico es el mil milon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('AMLO Tweets:')\n",
    "print()\n",
    "for i in range(10):\n",
    "  generate_AMLO_tweet(temperature=0.3)\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea4_victor_gomez.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
